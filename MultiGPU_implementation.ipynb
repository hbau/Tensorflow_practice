{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2jxx2JXA1-H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU implementation: set GPU size to 4 with MirroredStrategy \n",
        "# This will use all the GPUs that are visible to TensorFlow\n",
        "os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "##############################################################################\n",
        "# If only need a fraction of the GPUs, replace the MirroredStrategy instance #\n",
        "# \"strategy\" with \"fraction_strategy\" through out the remaining of the codes.#\n",
        "##############################################################################\n",
        "# fraction_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mt0pNkeBMZI",
        "outputId": "91deb3b2-972a-4112-f08b-9af43507b338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:0,/job:localhost/replica:0/task:0/device:GPU:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "# Normalize the images to [0, 1] range.\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)\n",
        "\n",
        "# Batch the input data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "# Create Datasets from the batches\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "# Create Distributed Datasets from the datasets\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "-S_V1LF2B4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model architecture\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "SoH5MpuhCIu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    # Use sparse categorical crossentropy \n",
        "    # Set reduction to `none` to do the reduction afterwards and divide byglobal batch size.\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def compute_loss(labels, predictions):\n",
        "        # Per_example_loss will have an entry per GPU -- i.e. the loss for each replica\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        print(per_example_loss)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    # Getting the average of the losses\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "    # Check accuracy\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "    # Optimizer will be Adam\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Create the model within the scope\n",
        "    model = create_model()"
      ],
      "metadata": {
        "id": "UCfTD5kdCOM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `run` replicates with the distributed input.\n",
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "  #tf.print(per_replica_losses.values)\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "def train_step(inputs):\n",
        "  images, labels = inputs\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training=True)\n",
        "    loss = compute_loss(labels, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_accuracy.update_state(labels, predictions)\n",
        "  return loss\n",
        "\n",
        "##############\n",
        "# Test Steps #\n",
        "##############\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "  return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "def test_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss.update_state(t_loss)\n",
        "  test_accuracy.update_state(labels, predictions)\n"
      ],
      "metadata": {
        "id": "58RNhh8fCXHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  # Do Training\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for batch in train_dist_dataset:\n",
        "    total_loss += distributed_train_step(batch)\n",
        "    num_batches += 1\n",
        "  train_loss = total_loss / num_batches\n",
        "\n",
        "  # Do Testing\n",
        "  for batch in test_dist_dataset:\n",
        "    distributed_test_step(batch)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n",
        "\n",
        "  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
        "\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dUao4j-Cc95",
        "outputId": "35ec7af6-f7c5-43a2-f953-42ebcbef2b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Epoch 1, Loss: 0.5171646475791931, Accuracy: 81.32666778564453, Test Loss: 0.4025465250015259, Test Accuracy: 85.6500015258789\n",
            "Epoch 2, Loss: 0.33958813548088074, Accuracy: 87.75499725341797, Test Loss: 0.34390920400619507, Test Accuracy: 87.7300033569336\n",
            "Epoch 3, Loss: 0.29120761156082153, Accuracy: 89.30332946777344, Test Loss: 0.3059218227863312, Test Accuracy: 88.86000061035156\n",
            "Epoch 4, Loss: 0.2624373733997345, Accuracy: 90.4816665649414, Test Loss: 0.2913820445537567, Test Accuracy: 89.38999938964844\n",
            "Epoch 5, Loss: 0.23675400018692017, Accuracy: 91.19166564941406, Test Loss: 0.26277869939804077, Test Accuracy: 90.55999755859375\n",
            "Epoch 6, Loss: 0.21798725426197052, Accuracy: 92.00833129882812, Test Loss: 0.2586404085159302, Test Accuracy: 90.58000183105469\n",
            "Epoch 7, Loss: 0.1993543654680252, Accuracy: 92.60166931152344, Test Loss: 0.26550689339637756, Test Accuracy: 90.34000396728516\n",
            "Epoch 8, Loss: 0.18069536983966827, Accuracy: 93.32167053222656, Test Loss: 0.2679639160633087, Test Accuracy: 90.52999877929688\n",
            "Epoch 9, Loss: 0.16798585653305054, Accuracy: 93.72833251953125, Test Loss: 0.25945308804512024, Test Accuracy: 90.91999816894531\n",
            "Epoch 10, Loss: 0.15213128924369812, Accuracy: 94.37833404541016, Test Loss: 0.2590111196041107, Test Accuracy: 91.08999633789062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcLlWJCPCf0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}